{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Function to get the target column\n",
    "def get_target_column(file_path):\n",
    "  \"\"\"\n",
    "  Get the name of the target column from the CSV file.\n",
    "  \n",
    "  Parameters:\n",
    "  file_path (str): Path to the CSV file.\n",
    "  \n",
    "  Returns:\n",
    "  str: Name of the target column.\"\"\"\n",
    "  return df.columns[-1]\n",
    "# Function to load the dataset\n",
    "def load_data(file_path):\n",
    "    \"\"\"\n",
    "    Load the dataset from the given file path.\n",
    "    \n",
    "    Parameters:\n",
    "    file_path (str): Path to the CSV file.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: Loaded dataset.\n",
    "    \"\"\"\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "# Function to display basic information about the dataset\n",
    "def display_basic_info(df):\n",
    "    \"\"\"\n",
    "    Display basic information about the dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The dataset to inspect.\n",
    "    \"\"\"\n",
    "    print(\"Dataset Info:\")\n",
    "    print(df.info())\n",
    "\n",
    "# Function to check for missing values\n",
    "def check_missing_values(df):\n",
    "    \"\"\"\n",
    "    Check for missing values in the dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The dataset to check.\n",
    "    \n",
    "    Returns:\n",
    "    pd.Series: Missing values count for each column.\n",
    "    \"\"\"\n",
    "    return df.isnull().sum()\n",
    "\n",
    "# Function to display summary statistics of the dataset\n",
    "def display_summary_statistics(df):\n",
    "    \"\"\"\n",
    "    Display summary statistics of the dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The dataset to summarize.\n",
    "    \"\"\"\n",
    "    print(\"\\nSummary Statistics:\")\n",
    "    print(df.describe())\n",
    "\n",
    "# Function to visualize missing values\n",
    "def plot_missing_values(df):\n",
    "    \"\"\"\n",
    "    Plot a heatmap of missing values in the dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The dataset to visualize.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.heatmap(df.isnull(), cmap='viridis', cbar=False, yticklabels=False)\n",
    "    plt.title(\"Missing Values Heatmap\")\n",
    "    plt.show()\n",
    "\n",
    "# Function to plot the distribution of the target variable (e.g., DON concentration)\n",
    "def plot_target_distribution(df, target_column):\n",
    "    \"\"\"\n",
    "    Plot the distribution of the target variable.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The dataset containing the target column.\n",
    "    target_column (str): The name of the target column.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.histplot(df[target_column], bins=30, kde=True)\n",
    "    plt.title(\"Distribution of DON Concentration\")\n",
    "    plt.xlabel(\"DON Concentration\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.show()\n",
    "\n",
    "# Function to visualize outliers in spectral features\n",
    "def plot_boxplot(df):\n",
    "    \"\"\"\n",
    "    Plot a boxplot to detect outliers in spectral features.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The dataset to visualize.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.boxplot(data=df.iloc[:, :-1])  # Assuming last column is the target\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.title(\"Boxplot of Spectral Features\")\n",
    "    plt.show()\n",
    "\n",
    "# Main function to execute the data exploration\n",
    "def data_exploration(file_path, target_column):\n",
    "    \"\"\"\n",
    "    Perform data exploration by loading the data, checking for missing values,\n",
    "    displaying basic info, and visualizing key aspects.\n",
    "    \n",
    "    Parameters:\n",
    "    file_path (str): Path to the CSV file.\n",
    "    target_column (str): The name of the target column.\n",
    "    \"\"\"\n",
    "    # Load dataset\n",
    "    df = load_data(file_path)\n",
    "    \n",
    "    # Display basic info\n",
    "    display_basic_info(df)\n",
    "    \n",
    "    # Check for missing values\n",
    "    missing_values = check_missing_values(df)\n",
    "    print(\"\\nMissing Values:\")\n",
    "    print(missing_values)\n",
    "    \n",
    "    # Display summary statistics\n",
    "    display_summary_statistics(df)\n",
    "    \n",
    "    # Visualize missing values\n",
    "    plot_missing_values(df)\n",
    "    \n",
    "    # Plot target distribution\n",
    "    plot_target_distribution(df, target_column)\n",
    "    \n",
    "    # Visualize outliers in spectral features\n",
    "    plot_boxplot(df)\n",
    "\n",
    "file_path = \"/Users/srinivaskalyan/Downloads/ImageAI/MLE-Assignment.csv\"  # Update this with your actual file path\n",
    "target_column = get_target_column(file_path)  # Replace with the actual name of the target column\n",
    "data_exploration(file_path, target_column)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Function to load data\n",
    "def load_data(file_path):\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "# Function to handle missing values\n",
    "def handle_missing_values(df):\n",
    "    df = df.copy()\n",
    "    df.dropna(inplace=True)  # Drop rows with missing values\n",
    "    return df\n",
    "\n",
    "# Function to detect and remove outliers using IQR\n",
    "def remove_outliers(df):\n",
    "    df = df.copy()\n",
    "    Q1 = df.iloc[:, 1:-1].quantile(0.25)\n",
    "    Q3 = df.iloc[:, 1:-1].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    mask = ~((df.iloc[:, 1:-1] < lower_bound) | (df.iloc[:, 1:-1] > upper_bound)).any(axis=1)\n",
    "    return df[mask]\n",
    "\n",
    "# Function to cap outliers (Winsorization)\n",
    "def cap_outliers(df):\n",
    "    df = df.copy()\n",
    "    Q1 = df.iloc[:, 1:-1].quantile(0.25)\n",
    "    Q3 = df.iloc[:, 1:-1].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    df.iloc[:, 1:-1] = np.where(df.iloc[:, 1:-1] < lower_bound, lower_bound, df.iloc[:, 1:-1])\n",
    "    df.iloc[:, 1:-1] = np.where(df.iloc[:, 1:-1] > upper_bound, upper_bound, df.iloc[:, 1:-1])\n",
    "    return df\n",
    "\n",
    "# # Function to normalize spectral data\n",
    "# def normalize_data(df):\n",
    "#     df.iloc[:, 1:-1] = (df.iloc[:, 1:-1] - df.iloc[:, 1:-1].min()) / (df.iloc[:, 1:-1].max() - df.iloc[:, 1:-1].min())\n",
    "#     return df\n",
    "\n",
    "# Function to plot boxplots before and after outlier removal\n",
    "def plot_outliers(df_before, df_after, df_capped):\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    sns.boxplot(data=df_before.iloc[:, 1:-1], ax=axes[0]).set_title(\"Before Outlier Removal\")\n",
    "    sns.boxplot(data=df_after.iloc[:, 1:-1], ax=axes[1]).set_title(\"After Outlier Removal\")\n",
    "    sns.boxplot(data=df_capped.iloc[:, 1:-1], ax=axes[2]).set_title(\"After Capping Outliers\")\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()\n",
    "\n",
    "# Function to plot mean spectral reflectance\n",
    "def plot_mean_reflectance(df):\n",
    "    mean_reflectance = df.iloc[:, 1:-1].mean()\n",
    "    std_reflectance = df.iloc[:, 1:-1].std()\n",
    "    wavelengths = df.columns[1:-1]\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(wavelengths, mean_reflectance, label=\"Mean Reflectance\", color='b')\n",
    "    plt.fill_between(wavelengths, mean_reflectance - std_reflectance, mean_reflectance + std_reflectance, color='b', alpha=0.2)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.xlabel(\"Wavelength\")\n",
    "    plt.ylabel(\"Normalized Reflectance\")\n",
    "    plt.title(\"Average Spectral Reflectance with Standard Deviation\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Function to plot feature correlation heatmap\n",
    "def plot_correlation_heatmap(df):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(df.iloc[:, 1:-1].corr(), cmap=\"coolwarm\", annot=False, fmt=\".2f\", linewidths=0.5)\n",
    "    plt.title(\"Feature Correlation Heatmap\")\n",
    "    plt.show()\n",
    "\n",
    "# Main function to execute preprocessing\n",
    "def preprocess_data(file_path):\n",
    "    df = load_data(file_path)\n",
    "    df = handle_missing_values(df)\n",
    "    df_no_outliers = remove_outliers(df)\n",
    "    df_capped = cap_outliers(df)\n",
    "    # df_normalized = normalize_data(df_capped)\n",
    "    \n",
    "    print(f\"Original dataset size: {df.shape[0]} samples\")\n",
    "    print(f\"Cleaned dataset size (outliers removed): {df_no_outliers.shape[0]} samples\")\n",
    "    print(f\"Samples removed: {df.shape[0] - df_no_outliers.shape[0]}\")\n",
    "    \n",
    "    plot_outliers(df, df_no_outliers, df_capped)\n",
    "    plot_mean_reflectance(df_capped)\n",
    "    plot_correlation_heatmap(df_capped)\n",
    "    \n",
    "    return df_capped\n",
    "\n",
    "df_preprocessed = preprocess_data(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import joblib\n",
    "\n",
    "# Function to normalize data using Min-Max Scaling\n",
    "def normalize_data(X):\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    joblib.dump(scaler, 'minmax_scaler.pkl')  # Save the scaler for future use\n",
    "    return X_scaled\n",
    "\n",
    "# Function to apply PCA\n",
    "def apply_pca(X):\n",
    "    pca = PCA(n_components=X.shape[1])\n",
    "    X_pca = pca.fit_transform(X)\n",
    "    joblib.dump(pca, 'pca_model_best.pkl')  # Save PCA model\n",
    "    return X_pca\n",
    "\n",
    "# Function to create the neural network model\n",
    "def build_model(input_dim):\n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', input_shape=(input_dim,)),\n",
    "        Dropout(0.2),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1)  # Regression output\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# Function to train and evaluate the model\n",
    "def train_and_evaluate_model(X_train, X_test, y_train, y_test):\n",
    "    model = build_model(X_train.shape[1])\n",
    "    \n",
    "    history = model.fit(X_train, y_train, epochs=100, batch_size=16, \n",
    "                        validation_data=(X_test, y_test), verbose=1)\n",
    "\n",
    "    # Predict on test data\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Compute evaluation metrics\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"Neural Network Performance:\")\n",
    "    print(f\"MAE: {mae:.2f}, RMSE: {rmse:.2f}, RÂ²: {r2:.4f}\")\n",
    "\n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('MSE Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Training vs Validation Loss')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['mae'], label='Train MAE')\n",
    "    plt.plot(history.history['val_mae'], label='Validation MAE')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('MAE')\n",
    "    plt.legend()\n",
    "    plt.title('Training vs Validation MAE')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    model.save('/content/don_concentration_predictor.h5')\n",
    "    print(\"Model saved successfully!\")\n",
    "\n",
    "X = df_preprocessed.iloc[:, 1:-1].values  # Spectral features\n",
    "y = df_preprocessed.iloc[:, -1].values    # Target (DON concentration)\n",
    "\n",
    "# Normalize Data\n",
    "X_scaled = normalize_data(X)\n",
    "# Apply PCA\n",
    "X_pca = apply_pca(X_scaled)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train and evaluate the model\n",
    "train_and_evaluate_model(X_train, X_test, y_train, y_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Random Forest and XG Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_random_forest(X_train, X_test, y_train, y_test):\n",
    "    rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    rf.fit(X_train, y_train)\n",
    "    y_pred = rf.predict(X_test)\n",
    "    \n",
    "    return rf, y_pred\n",
    "\n",
    "# Function to train an XGBoost model\n",
    "def train_xgboost(X_train, X_test, y_train, y_test):\n",
    "    xgb = XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "    xgb.fit(X_train, y_train)\n",
    "    y_pred = xgb.predict(X_test)\n",
    "\n",
    "    return xgb, y_pred\n",
    "\n",
    "rf_model, y_pred_rf = train_random_forest(X_train, X_test, y_train, y_test)\n",
    "evaluate_model(y_test, y_pred_rf, \"Random Forest\")\n",
    "\n",
    "# Train and evaluate XGBoost\n",
    "xgb_model, y_pred_xgb = train_xgboost(X_train, X_test, y_train, y_test)\n",
    "evaluate_model(y_test, y_pred_xgb, \"XGBoost\")\n",
    "\n",
    "# Save models\n",
    "joblib.dump(rf_model, 'random_forest_model.pkl')\n",
    "joblib.dump(xgb_model, 'xgboost_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "\n",
    "# Load the XGBoost model\n",
    "xgb_model = joblib.load('xgboost_model.pkl')\n",
    "\n",
    "# Load the PCA model\n",
    "pca = joblib.load('/content/pca_model_best.pkl')\n",
    "\n",
    "# Step 2: Example new data (500 features)\n",
    "new_spectrum = np.random.rand(500)\n",
    "\n",
    "# Step 3: Apply PCA to reduce the features to 448\n",
    "if new_spectrum.shape[0] > 448:\n",
    "    new_spectrum = new_spectrum[:448]  # Truncate the input to 448 features if it's larger\n",
    "transformed_spectrum = pca.transform(new_spectrum.reshape(1, -1))\n",
    "\n",
    "# Step 4: Make prediction using the loaded XGBoost model\n",
    "prediction_xgb = xgb_model.predict(transformed_spectrum)\n",
    "prediction_xgb_value = float(prediction_xgb[0])\n",
    "print(f\"XGBoost Prediction: {prediction_xgb_value}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
